{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.1"
    },
    "latex_envs": {
      "LaTeX_envs_menu_present": true,
      "autoclose": false,
      "autocomplete": true,
      "bibliofile": "biblio.bib",
      "cite_by": "apalike",
      "current_citInitial": 1,
      "eqLabelWithNumbers": true,
      "eqNumInitial": 1,
      "hotkeys": {
        "equation": "Ctrl-E",
        "itemize": "Ctrl-I"
      },
      "labels_anchors": false,
      "latex_user_defs": false,
      "report_style_numbering": false,
      "user_envs_cfg": false
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {
        "height": "calc(100% - 180px)",
        "left": "10px",
        "top": "150px",
        "width": "427px"
      },
      "toc_section_display": true,
      "toc_window_display": true
    },
    "colab": {
      "name": "Homework 1.ipynb",
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1jmdmhAjkHq"
      },
      "source": [
        "\n",
        "\n",
        "# Developing Fair Classifiers: A case study using COMPAS data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1fhIA_YjkHw"
      },
      "source": [
        "## Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RKUzjX6jkHw"
      },
      "source": [
        "##### There are many different ways to measure fairness in AI. As such, there are many ways to adjust your AI models so that they better fit these fairness metrics. Here, you will examine this firsthand with the COMPAS dataset, which is a dataset that has been widely used to predict inmates' likelihood of recidivism (reoffense). First, you will run a simple classifier on this dataset and examine the results. Then, you will be asked how this classifier could be made more fair and, for extra credit, you can try implementing these changes yourself.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUSqhNCvjkHw"
      },
      "source": [
        "## Dataset exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "043ta_lx65N7"
      },
      "source": [
        "##### First, you will run the cells and answer the questions below in order to explore the contents of the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_BVlQfdjkHw"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57JEWrz1jkHx"
      },
      "source": [
        "#uncomment these lines if you need to install either of these two packages\n",
        "\n",
        "#!pip install plotly\n",
        "#!pip install aif360[LFR]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JbNBFnJvjkHy"
      },
      "source": [
        "import urllib\n",
        "import urllib.request\n",
        "import os,sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from functools import reduce\n",
        "pd.set_option('display.max_columns',None)\n",
        "from pprint import pprint\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn.metrics as performance\n",
        "from sklearn.preprocessing import scale\n",
        "from random import seed, shuffle\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "try:\n",
        "    import plotly.express as px\n",
        "    import plotly.figure_factory as ff\n",
        "    import plotly.io as pio\n",
        "    import plotly.graph_objects as go\n",
        "    pio.renderers.default = \"colab\" #change to notebook_connected if in a Jupyter notebook\n",
        "except ImportError as e:\n",
        "#     !conda install --yes --prefix {sys.prefix} -c plotly plotly-orca==1.2.1 psutil requests\n",
        "    print(\"plotly is not installed !! please install the package to be able to render plotly visualizations\")\n",
        "\n",
        "\n",
        "try:\n",
        "    from aif360.metrics import BinaryLabelDatasetMetric\n",
        "    from aif360.metrics import ClassificationMetric\n",
        "    from aif360.metrics.utils import compute_boolean_conditioning_vector\n",
        "    from aif360.algorithms.postprocessing.calibrated_eq_odds_postprocessing import CalibratedEqOddsPostprocessing\n",
        "\n",
        "    from aif360.algorithms.preprocessing.optim_preproc_helpers.data_preproc_functions\\\n",
        "                    import load_preproc_data_compas\n",
        "    \n",
        "except ImportError as e:\n",
        "    print('please install aif360 package to proceed with the fairness section')\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vM-K4B-FjkHy"
      },
      "source": [
        "### Setting up visualization properties"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "G1DOUO0gjkHy"
      },
      "source": [
        "SMALL_SIZE = 8\n",
        "MEDIUM_SIZE = 10\n",
        "BIGGER_SIZE = 12\n",
        "fig=plt.figure(figsize=(10,6))\n",
        "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
        "plt.rc('axes', titlesize=MEDIUM_SIZE)     # fontsize of the axes title\n",
        "plt.rc('axes', labelsize=14)    # fontsize of the x and y labels\n",
        "plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
        "plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
        "plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
        "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfjK9V_tjkH1"
      },
      "source": [
        "### UTILS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3nX5ZHNHjkH2"
      },
      "source": [
        "def hist(column_name, x):\n",
        "    '''\n",
        "    helper function to plot historgram for a dataframe \n",
        "    '''\n",
        "    return px.histogram(column_name, x = x,\n",
        "                       title = x,\n",
        "                       opacity = 0.8,\n",
        "                       color_discrete_sequence = ['indianred'])\n",
        "def plot_roc(fpr,tpr,thresholds=None,label=None):\n",
        "    '''\n",
        "    plot roc curve for multiple instances.\n",
        "    '''\n",
        "    \n",
        "    colors = ['red','blue','aqua', 'darkorange', 'cornflowerblue', 'burlywood', 'lightsalmon', 'olive']\n",
        "\n",
        "    plt.figure(figsize=(12,8))\n",
        "    lw = 2\n",
        "    for i in range(len(fpr)):\n",
        "        plt.plot(fpr[i], tpr[i], color=colors[i],label=label[i],\n",
        "             lw=lw)\n",
        "        \n",
        "     \n",
        "    plt.plot([0, 1], [0, 1], color='black', lw=lw, linestyle='--')\n",
        "      \n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('ROC Curves of attributes ')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3wdcXNYjkH2"
      },
      "source": [
        "### Download data file and dependency files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0iONiuLnjkH2"
      },
      "source": [
        "SEED = 1234\n",
        "seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "def check_data_file(fname):\n",
        "    files = os.listdir(\".\") # get the current directory listing\n",
        "    print(\"Looking for file %s in the current directory...\" % (fname))\n",
        "\n",
        "    if fname not in files:\n",
        "        print(\" %s not found! Downloading from GitHub...\" % fname)\n",
        "        addr = \"https://raw.githubusercontent.com/propublica/compas-analysis/master/compas-scores-two-years.csv\"\n",
        "        response = urllib.request.urlopen(addr)\n",
        "        data = response.read()\n",
        "        fileOut = open(fname, \"wb\")\n",
        "        fileOut.write(data)\n",
        "        fileOut.close()\n",
        "        print(\"%s download and saved locally..\" % fname)\n",
        "    else:\n",
        "        print(\"File found in current directory..\")\n",
        "    \n",
        "COMPAS_INPUT_FILE = \"compas-scores-two-years.csv\"\n",
        "check_data_file(COMPAS_INPUT_FILE)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "J1AOWe3EjkH3"
      },
      "source": [
        "### Load and clean data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "DwTsFy-DjkH3"
      },
      "source": [
        "dataset = pd.read_csv(COMPAS_INPUT_FILE)\n",
        "\n",
        "dataset = dataset.dropna(subset=[\"days_b_screening_arrest\"]) # dropping missing vals\n",
        "\n",
        "dataset = dataset[ (dataset.days_b_screening_arrest <= 30) &\n",
        "(dataset.days_b_screening_arrest >= -30) &\n",
        "(dataset.is_recid != -1) & (dataset.c_charge_degree != 'O') & (dataset.score_text != 'N/A') ]\n",
        "\n",
        "dataset.reset_index(inplace=True, drop=True) # renumber the rows from 0 again"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "scrolled": false,
        "id": "GRkkfYKAjkH3"
      },
      "source": [
        "dataset.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kG49svx1jkH4"
      },
      "source": [
        "### Basic analysis of demographics\n",
        "A series of plots is given below which look at the distribution of the data, the decile scores (the scores in the COMPAS dataset that measure likelihood of reoffense), and the risk category, which is based on the decile score and is determined in the following manner:\n",
        "\n",
        " 1 – 4: scale score is low relative to other offenders in norm group.\n",
        "\n",
        " 5 – 7: scale score is medium relative to other offenders in norm group.\n",
        "\n",
        " 8 – 10: scale score is high relative to other offenders in norm group.\n",
        "\n",
        "\n",
        "After looking at the plots, you will be asked to describe the imbalances of the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "X7Q-o2A8jkH4"
      },
      "source": [
        "def get_basic_stats(column_name,check_values=None,\n",
        "                    check_NaN=False,group_count=True,plot_hist = False):\n",
        "    '''\n",
        "    basic stats on a given column of the dataset\n",
        "    @param column_name: column name to compute stats on\n",
        "    '''\n",
        "    \n",
        "    result = {}\n",
        "    \n",
        "    if column_name not in dataset.columns:\n",
        "        raise ValueError(\"column_name must be set to a value from the the available values\")\n",
        "        \n",
        "    #values in the column\n",
        "    column_values = np.unique(dataset[column_name].values)\n",
        "    \n",
        "    if check_values:\n",
        "        print('-',column_name,\"in dataset:\")\n",
        "        [print(value) for value in column_values]\n",
        "\n",
        "    if check_NaN:\n",
        "        print(\"-NaN  present in the column?\",dataset[column_name].isnull().any())\n",
        "    \n",
        "    if group_count:\n",
        "        values_count = dataset[column_name].value_counts()   \n",
        "        result.update(values_count=values_count)\n",
        "        \n",
        "    #histogram of values\n",
        "    if plot_hist:\n",
        "        fig = hist(dataset[column_name], column_name) \n",
        "        fig.show()        \n",
        "    \n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6J1JjYj9jkH4"
      },
      "source": [
        "result = get_basic_stats(\"race\",plot_hist=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "QpVT42IAjkH5"
      },
      "source": [
        "result = get_basic_stats(\"age\", plot_hist=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsEIRtpnjkH5"
      },
      "source": [
        "sex_race_df = pd.crosstab(dataset.race, dataset.sex)\n",
        "sex_race_df = sex_race_df.stack().reset_index().rename(columns={0:\"value\"})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2znoGtOjkH5"
      },
      "source": [
        "race = dataset.race.unique()\n",
        "males = sex_race_df.value[sex_race_df.sex == 'Male']\n",
        "females = sex_race_df.value[sex_race_df.sex == 'Female']\n",
        "\n",
        "fig = go.Figure(data=[\n",
        "    go.Bar(name='Male', x=race, y= males),\n",
        "    go.Bar(name='Female', x=race, y=females)\n",
        "])\n",
        "# Change the bar mode\n",
        "fig.update_layout(barmode='stack')\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "ccH85bhdjkH6"
      },
      "source": [
        "decile_score_per_race = pd.crosstab(dataset.decile_score, dataset.race)\n",
        "decile_score_per_race = decile_score_per_race.stack().reset_index().rename(columns = {0:'values'})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "MWWO5wX0jkH6"
      },
      "source": [
        "dec_score = dataset.decile_score.unique()\n",
        "afrac_amer = decile_score_per_race.values[decile_score_per_race.race == \"African-American\"][:,2]\n",
        "hispanic = decile_score_per_race.values[decile_score_per_race.race == \"Hispanic\"][:,2]\n",
        "asian = decile_score_per_race.values[decile_score_per_race.race == \"Asian\"][:,2]\n",
        "causcasian = decile_score_per_race.values[decile_score_per_race.race == \"Caucasian\"][:,2]\n",
        "native = decile_score_per_race.values[decile_score_per_race.race == \"Native American\"][:,2]\n",
        "other = decile_score_per_race.values[decile_score_per_race.race == \"Other\"][:,2]\n",
        "\n",
        "fig = go.Figure(data = [\n",
        "    go.Bar(name = 'African-American', x = dec_score, y = afrac_amer),\n",
        "    go.Bar(name = 'Hispanic', x = dec_score, y = hispanic),\n",
        "    go.Bar(name = 'Asian', x = dec_score, y = asian),\n",
        "    go.Bar(name = 'Causcasian', x = dec_score, y = causcasian),\n",
        "    go.Bar(name = 'Native-American', x = dec_score, y = native),\n",
        "    go.Bar(name = 'Other', x = dec_score, y = other)\n",
        "])\n",
        "fig.update_layout(barmode='stack')\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRyYvb_XjkH7"
      },
      "source": [
        "The *low*, *medium* and *high* values below are categories for risk based on the **decile_score** computed. This will be discussed in more detail in the upcoming sections."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "SzgksdT3jkH7"
      },
      "source": [
        "score_text_per_race = pd.crosstab(dataset.score_text, dataset.race)\n",
        "score_text_per_race = score_text_per_race.stack().reset_index().rename(columns = {0:'values'})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIG87rosjkH7"
      },
      "source": [
        "dec_score = dataset.score_text.unique()\n",
        "afrac_amer = score_text_per_race.values[score_text_per_race.race == \"African-American\"][:,2]\n",
        "hispanic = score_text_per_race.values[score_text_per_race.race == \"Hispanic\"][:,2]\n",
        "asian = score_text_per_race.values[score_text_per_race.race == \"Asian\"][:,2]\n",
        "causcasian = score_text_per_race.values[score_text_per_race.race == \"Caucasian\"][:,2]\n",
        "native = score_text_per_race.values[score_text_per_race.race == \"Native American\"][:,2]\n",
        "other = score_text_per_race.values[score_text_per_race.race == \"Other\"][:,2]\n",
        "\n",
        "fig = go.Figure(data = [\n",
        "    go.Bar(name = 'African-American', x = dec_score, y = afrac_amer),\n",
        "    go.Bar(name = 'Hispanic', x = dec_score, y = hispanic),\n",
        "    go.Bar(name = 'Asian', x = dec_score, y = asian),\n",
        "    go.Bar(name = 'Causcasian', x = dec_score, y = causcasian),\n",
        "    go.Bar(name = 'Native-American', x = dec_score, y = native),\n",
        "    go.Bar(name = 'Other', x = dec_score, y = other)\n",
        "])\n",
        "fig.update_layout(barmode='stack')\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8KEGlKLjkH7"
      },
      "source": [
        "### Decile score vs Race\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IUnpp9LgjkH7"
      },
      "source": [
        "def scores_stats(category, value):\n",
        "    '''\n",
        "    @param category: vategory to perform stats on\n",
        "    @param value: entry in the specefied category\n",
        "    '''\n",
        "    scores = dataset[dataset[category] == value]\n",
        "    scores = scores[['decile_score','score_text']]\n",
        "    histogram,_ = np.histogram(scores['decile_score'])\n",
        "    fig = px.histogram(scores['decile_score'], 'decile_score',\n",
        "                       title = 'Decile score of '+ value + 's',\n",
        "                       opacity = 0.8,\n",
        "                       color_discrete_sequence = ['indianred'])\n",
        "    fig.add_shape(dict(\n",
        "        type=\"line\",\n",
        "        x0 = scores['decile_score'].mean(),\n",
        "        y0 = 0,\n",
        "        x1 = scores['decile_score'].mean(),\n",
        "        y1 = max(histogram) + 20,\n",
        "        line=dict(\n",
        "            color = \"LightSeaGreen\",\n",
        "            width = 4,\n",
        "            dash = \"dashdot\")\n",
        "    ))\n",
        "    fig.show()\n",
        "    print('mean decile score: %.2f'%scores['decile_score'].mean())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTqEh1YtjkH7"
      },
      "source": [
        "scores_stats('race','African-American')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5KyTigcjkH8"
      },
      "source": [
        "scores_stats('race','Caucasian')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4lMZG3zSjkH8"
      },
      "source": [
        "### Decile score vs Gender\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "_g5rz9v-jkH8"
      },
      "source": [
        "scores_stats('sex','Male')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4o_1YFSMjkH8"
      },
      "source": [
        "scores_stats('sex','Female')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ldPTjIuExHC"
      },
      "source": [
        "### **Question 1: Takeaways from the data**\n",
        "Now that you have looked at the way this data is distributed, what do you notice? What imbalances (if any) do you see and how might they affect the results of a model that uses this dataset? Answer these questions in the cell below:\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6X01SplFcVc"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBTmS2j1jkH9"
      },
      "source": [
        "### Performance analysis of COMPAS\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5XPlttgjkH9"
      },
      "source": [
        "Now, we are going to measure COMPAS's overall performance and then break it down by gender and race. We do this by comparing the COMPAS scores with the ground truth data regarding whether or not a person was re-arrested for an offense within two years (what COMPAS is trying to predict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FyilqZdwjkH9"
      },
      "source": [
        "#### Number of people who were re-arrested\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9eyF05yjkH9"
      },
      "source": [
        "#people whom got re-arrested in two years span\n",
        "print('Number of people in the dataset whom got re-arrested in the span of two years: ',\n",
        "      dataset['two_year_recid'].sum())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBzw2KnujkH9"
      },
      "source": [
        "#### Number of people who were rearrested, split by race and gender"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E13bVLjhjkH9"
      },
      "source": [
        "# recedivism by race\n",
        "cau_sum = dataset[(dataset['race'] == 'Caucasian') & (dataset['two_year_recid']==1)]['two_year_recid'].sum()\n",
        "print('Number of Caucasians in the dataset whom got re-arrested in the span of two years and \\\n",
        "also predicted to recid: {0}/{1}'.format(cau_sum,dataset['race'].value_counts()['Caucasian']))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLa2btc3jkH-"
      },
      "source": [
        "am_sum = dataset[(dataset['race'] == 'African-American') & (dataset['two_year_recid']==1)]['two_year_recid'].sum()\n",
        "print('Number of African Americans in the dataset whom got re-arrested in the span of two years and \\\n",
        "also predicted to recid: {0}/{1}'.format(am_sum,dataset['race'].value_counts()['African-American']))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LD6Us2hyjkH-"
      },
      "source": [
        "#recedivism by gender\n",
        "male_sum = dataset[(dataset['sex'] == 'Male') & (dataset['two_year_recid']==1)]['two_year_recid'].sum()\n",
        "print('Number of males in the dataset whom got re-arrested in the span of two years and \\\n",
        "also predicted to recid: {0}/{1}'.format(male_sum,dataset['sex'].value_counts()['Male']))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "wGkxHsodjkH-"
      },
      "source": [
        "female_sum = dataset[(dataset['sex'] == 'Female') & (dataset['two_year_recid']==1)]['two_year_recid'].sum()\n",
        "print('Number of females in the dataset whom got re-arrested in the span of two years and \\\n",
        "also predicted to recid:{0}/{1}'.format(female_sum,dataset['sex'].value_counts()['Female']))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_OwdQs2tjkH_"
      },
      "source": [
        "two_recide_sex_race_df = pd.crosstab(dataset.race[dataset.two_year_recid == 1], dataset.sex[dataset.two_year_recid == 1])\n",
        "two_recide_sex_race_df = two_recide_sex_race_df.stack().reset_index().rename(columns={0:\"value\"})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "pYKrXmq5jkH_"
      },
      "source": [
        "race = dataset.race.unique()\n",
        "race = np.sort(race)\n",
        "males = two_recide_sex_race_df.value[two_recide_sex_race_df.sex == 'Male']\n",
        "females = two_recide_sex_race_df.value[two_recide_sex_race_df.sex == 'Female']\n",
        "\n",
        "fig = go.Figure(data=[\n",
        "    go.Bar(name='Male', x=race, y= males),\n",
        "    go.Bar(name='Female', x=race, y=females)\n",
        "], layout_title_text = \"Male/Female Recidivism by Race\")\n",
        "# Change the bar mode\n",
        "fig.update_layout(barmode='stack')\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCw5cptNjkH_"
      },
      "source": [
        "# Plot confusion matrix without passing the classifier\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from itertools import cycle, product\n",
        "def plot_confusion_matrix(cm, classes, title='Confusion Matrix', cmap=plt.cm.Blues):\n",
        "    print(cm)\n",
        "    \n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "    \n",
        "    fmt = 'd'\n",
        "    thresh = cm.max() / 2\n",
        "    for i, j in product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt),\n",
        "                horizontalalignment=\"center\",\n",
        "                color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    \n",
        "    \n",
        "def plot_conf_matrix(y_test, y_pred, class_names):\n",
        "    print(\"plotting the Confusion Matrix\")\n",
        "    cnf_matrix = confusion_matrix(y_test, y_pred)\n",
        "    np.set_printoptions(precision=2)\n",
        "    print('CCR = {}'.format(np.trace(cnf_matrix) / len(y_test)))\n",
        "    print('Precision = {}'.format(precision_macro_average(cnf_matrix)))\n",
        "    print('Recall = {}'.format(recall_macro_average(cnf_matrix)))\n",
        "    plt.figure()\n",
        "    plot_confusion_matrix(cnf_matrix, classes=class_names, title='Confusion Matrix')\n",
        "    plt.show()\n",
        "    \n",
        "def recall_macro_average(confusion_matrix):\n",
        "    rows, columns = confusion_matrix.shape\n",
        "    sum_of_recalls = 0\n",
        "    for label in range(columns):\n",
        "        sum_of_recalls += recall(label, confusion_matrix)\n",
        "    return sum_of_recalls / columns\n",
        "\n",
        "def precision_macro_average(confusion_matrix):\n",
        "    rows, columns = confusion_matrix.shape\n",
        "    sum_of_precisions = 0\n",
        "    for label in range(rows):\n",
        "        sum_of_precisions += precision(label, confusion_matrix)\n",
        "    return sum_of_precisions / rows\n",
        "\n",
        "def precision(label, confusion_matrix):\n",
        "    col = confusion_matrix[:, label]\n",
        "    return confusion_matrix[label, label] / col.sum()\n",
        "    \n",
        "def recall(label, confusion_matrix):\n",
        "    row = confusion_matrix[label, :]\n",
        "    return confusion_matrix[label, label] / row.sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "U8jMAte8jkIA"
      },
      "source": [
        "#mapping of low medium high to decile score\n",
        "print('decile score ranges in score text')\n",
        "scores = dataset[['decile_score','score_text']]\n",
        "low_range = scores[scores['score_text'] == 'Low']['decile_score']\n",
        "print('low:',low_range.min(),low_range.max())\n",
        "medium_range = scores[scores['score_text'] == 'Medium']['decile_score']\n",
        "print('medium:',medium_range.min(),medium_range.max())\n",
        "High_range = scores[scores['score_text'] == 'High']['decile_score']\n",
        "print('high',High_range.min(),High_range.max())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xhoeZufjkIA"
      },
      "source": [
        "To be able to compare compas scores in range [1,10], to the ground truth binary 0/1, the COMPAS scores are grouped into two groups to transfer them into binary labels\n",
        "- *recidivate* means *high* = label *1*\n",
        "- *will not recidivate* means *low/medium* = label *0*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emlLK4-djkIA"
      },
      "source": [
        "# we split scores to new classes, high to 1 and medium/ low to zero\n",
        "new_scores = dataset.copy()\n",
        "new_scores['decile_score'].replace(to_replace=range(1,8),value=0,inplace=True)\n",
        "new_scores['decile_score'].replace(to_replace=range(8,11),value=1,inplace=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKIyya28jkIB"
      },
      "source": [
        "#### Overall Accuracy of COMPAS Scores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QqtCzqrjkIB"
      },
      "source": [
        "overal_accuracy = performance.accuracy_score(new_scores['two_year_recid'],\n",
        "                                             new_scores['decile_score'],normalize=True)\n",
        "\n",
        "print('COMPAS overall performance against the true labels: {0:3f}%'.format(overal_accuracy *100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hide_input": true,
        "id": "orRbC45ojkIC"
      },
      "source": [
        "## Part 2.2 - Standard classifiers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_u4IUHNjkID"
      },
      "source": [
        "In this section, we are going to train different classifiers on the \"two_years_recid\" ground truth data and compare their performance, then plot the roc curves to investigate the bias in the classifiers towards different attributes. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VNQ0CwTjkID"
      },
      "source": [
        "- Preparing for the next part, we start using IBM aif360 package, a collection of fairness algorithms and performance metrics  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-vyz6nxTjkID"
      },
      "source": [
        "- We uses the following set of features out of the 137 questions collected by northepoint, we notice that we are including the charge degree(Felony/murder), the priors count(number of crimes commited before) nad age category."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vIUB1Pt_jkID"
      },
      "source": [
        "dataset_conditioned = load_preproc_data_compas()\n",
        "print(\"set of features to be used out of the dataset\")\n",
        "pprint(dataset_conditioned.feature_names)    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2lcWAVxjkID"
      },
      "source": [
        "- As a normalization step, all of the data values are turned into binary values, in order to equalize the effect of features with respect to the classifier.\n",
        "\n",
        "- The race has more than one value, but since we are only interested in the sensitive attributes (bias between African American and Caucasians, we drop the other races from the dataset, this does not in turn introduce a problem, since the size of the dataset drops from 6127 entry to 5278 entry."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-iEfCldjkID"
      },
      "source": [
        "dataset_as_df,_ = dataset_conditioned.convert_to_dataframe()\n",
        "print(\"Example samples from the dataset\")\n",
        "dataset_as_df.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kV6r9UdqjkIE"
      },
      "source": [
        "### Train-validate-test split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIHuG6PyjkIE"
      },
      "source": [
        "- We divide our dataset into 3 folds to train/test different classifers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VVEnZK_HjkIE"
      },
      "source": [
        "dataset_conditioned_train, dataset_conditioned_validate_test = dataset_conditioned.split([0.7], shuffle=True)\n",
        "# dataset_conditioned_validate, dataset_conditioned_test = dataset_conditioned_validate_test.split([0.5], shuffle=True)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktN8IwHZjkIE"
      },
      "source": [
        "print('size of traning and testing samples')\n",
        "dataset_conditioned_train.convert_to_dataframe()[0].shape,dataset_conditioned_validate_test.convert_to_dataframe()[0].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZyBnF_GdjkIE"
      },
      "source": [
        "### Splitting based on gender "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPiLmCq7jkIE"
      },
      "source": [
        "# get male samples from the dataset\n",
        "male_samples = dataset_as_df[dataset_as_df['sex'] == 1.0]\n",
        "male_samples_label = male_samples.filter(['two_year_recid'])\n",
        "male_samples.drop(columns=['two_year_recid'],inplace=True)\n",
        "\n",
        "# get femal samples from the dataset\n",
        "female_samples = dataset_as_df[dataset_as_df['sex'] == 0.0]\n",
        "female_samples_label = female_samples.filter(['two_year_recid'])\n",
        "female_samples.drop(columns=['two_year_recid'],inplace=True)\n",
        "print('number of female samples {0},number of male samples {1}'.format(female_samples.shape,\n",
        "                                                                                      male_samples.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHYYcwzSjkIE"
      },
      "source": [
        "### Splitting based on race - African American and Caucasian"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yP-1vKqWjkIF"
      },
      "source": [
        "\n",
        "#get caucasians samples from the dataset\n",
        "caucasian_samples = dataset_as_df[dataset_as_df['race'] == 1.0]\n",
        "caucasian_samples_label = caucasian_samples.filter(['two_year_recid'])\n",
        "caucasian_samples.drop(columns=['two_year_recid'],inplace=True)\n",
        "\n",
        "#get african american samples from the dataset\n",
        "african_american_samples = dataset_as_df[dataset_as_df['race'] == 0.0]\n",
        "african_american_samples_label = african_american_samples.filter(['two_year_recid'])\n",
        "african_american_samples.drop(columns=['two_year_recid'],inplace=True)\n",
        "print('number of african american samples {0},number of caucasians samples {1}'.format(african_american_samples.shape,\n",
        "                                                                                      caucasian_samples.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4woIPw9njkIF"
      },
      "source": [
        "### Accuracy of COMPAS Scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3r0skLaZjkIF"
      },
      "source": [
        "Before delving into the performance of different types of classifiers, we plot the ROC curves of compas scores with respect to race and gender in order to inspect the true positive rate and false positive rates."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6v_TU8CjkIF"
      },
      "source": [
        "#### ROC Curve - African American vs. Caucasian\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTIQ5Q46jkIF"
      },
      "source": [
        "fpr_AM,tpr_AM,thresholds_AM = performance.roc_curve(new_scores_african_american['two_year_recid'],\n",
        "                                           new_scores_african_american['decile_score'],pos_label=1)\n",
        "\n",
        "fpr_C,tpr_C,thresholds_C = performance.roc_curve(new_scores_Caucasian['two_year_recid'],\n",
        "                                           new_scores_Caucasian['decile_score'],pos_label=1)\n",
        "\n",
        "lw = 2\n",
        "fig=plt.figure(figsize=(10,6))\n",
        "plt.plot(fpr_AM, tpr_AM, color='darkorange',label='ROC curve of African americans',\n",
        "         lw=lw)\n",
        "plt.plot(fpr_C, tpr_C, color='r',\n",
        "         lw=lw,label='ROC curve of Caucasians')\n",
        "\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC curve')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLjABvThjkIF"
      },
      "source": [
        "#### ROC curve for gender"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nK_6aGkijkIF"
      },
      "source": [
        "fpr_female,tpr_female,thresholds_female = performance.roc_curve(new_scores_female['two_year_recid'],\n",
        "                                           new_scores_female['decile_score'],pos_label=1)\n",
        "\n",
        "fpr_male,tpr_male,thresholds_male = performance.roc_curve(new_scores_male['two_year_recid'],\n",
        "                                           new_scores_male['decile_score'],pos_label=1)\n",
        "\n",
        "lw = 2\n",
        "fig=plt.figure(figsize=(10,6))\n",
        "plt.plot(fpr_female, tpr_female, color='darkorange',label='ROC curve of females',\n",
        "         lw=lw)\n",
        "plt.plot(fpr_male, tpr_male, color='r',\n",
        "         lw=lw,label='ROC curve of males')\n",
        "\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC curve')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CktV3nVGqe-b"
      },
      "source": [
        "###**Question 2: What biases do you see in the COMPAS data based on these ROC curves?**\n",
        "Answer this question in the cell below"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPOHwvbzqxeQ"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-T0AKG-qCrQ"
      },
      "source": [
        "Next, we examine the results of running three simple classifiers on the COMPAS data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgIqgQvQjkIG"
      },
      "source": [
        "### SVM\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFG5NarwjkIG"
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "svm_classifier = SVC(gamma=0.001, C=100.)\n",
        "svm_classifier.fit(dataset_conditioned_train.features, dataset_conditioned_train.labels.ravel())\n",
        "\n",
        "pred = svm_classifier.predict(dataset_conditioned_validate_test.features)\n",
        "\n",
        "print('Accuracy of the SVM on testing set {0:.2f}% '\n",
        "      .format(performance.accuracy_score(dataset_conditioned_validate_test.labels, pred, normalize=True)*100))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3YuSu4ujkIG"
      },
      "source": [
        "#### ROC curve for race"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WbPuVcIVjkIG"
      },
      "source": [
        "# ROC curve for black vs caucasian\n",
        "fpr_svm_race,tpr_svm_race,label_svm_race=[],[],[]\n",
        "\n",
        "\n",
        "african_american_prediction = svm_classifier.predict(african_american_samples)\n",
        "\n",
        "fpr_AM,tpr_AM,thresholds_AM = performance.roc_curve(african_american_samples_label.to_numpy().ravel(),\n",
        "                                           african_american_prediction,pos_label=1)\n",
        "\n",
        "fpr_svm_race.append(fpr_AM)\n",
        "tpr_svm_race.append(tpr_AM)\n",
        "label_svm_race.append('ROC of african american')\n",
        "\n",
        "caucasian_prediction = svm_classifier.predict(caucasian_samples)\n",
        "fpr_C,tpr_C,thresholds_C = performance.roc_curve(caucasian_samples_label.to_numpy().ravel(),\n",
        "                                           caucasian_prediction,pos_label=1)\n",
        "fpr_svm_race.append(fpr_C)\n",
        "tpr_svm_race.append(tpr_C)\n",
        "label_svm_race.append('ROC of Caucasians')\n",
        "\n",
        "plot_roc(fpr_svm_race,tpr_svm_race,label=label_svm_race)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcy34QV4jkIG"
      },
      "source": [
        "#### ROC curve for gender"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yIYPw9WKjkIG"
      },
      "source": [
        "# ROC curve for male vs female\n",
        "fpr_svm_gender,tpr_svm_gender,label_svm_gender=[],[],[]\n",
        "\n",
        "\n",
        "female_prediction = svm_classifier.predict(female_samples)\n",
        "\n",
        "fpr_female,tpr_female,thresholds_female = performance.roc_curve(female_samples_label.to_numpy().ravel(),\n",
        "                                           female_prediction,pos_label=1)\n",
        "\n",
        "fpr_svm_gender.append(fpr_female)\n",
        "tpr_svm_gender.append(tpr_female)\n",
        "label_svm_gender.append('ROC of females')\n",
        "\n",
        "male_prediction = svm_classifier.predict(male_samples)\n",
        "fpr_male,tpr_male,thresholds_male = performance.roc_curve(male_samples_label.to_numpy().ravel(),\n",
        "                                           male_prediction,pos_label=1)\n",
        "fpr_svm_gender.append(fpr_male)\n",
        "tpr_svm_gender.append(tpr_male)\n",
        "label_svm_gender.append('ROC of male')\n",
        "\n",
        "plot_roc(fpr_svm_gender,tpr_svm_gender,label=label_svm_gender)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NAzSfxvEjkIH"
      },
      "source": [
        "### MLP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SyolLWpnjkIH"
      },
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "hidden_layer_grid = [(x,x) for x in range(10,16,2)]\n",
        "\n",
        "mlp_classifier = MLPClassifier()\n",
        "mlp_clf_opt = GridSearchCV(mlp_classifier,param_grid={'hidden_layer_sizes':hidden_layer_grid,\n",
        "                                                     'solver':['adam'],\n",
        "                                                      'learning_rate_init': [0.001],\n",
        "                                                      'learning_rate':['adaptive'],\n",
        "                                                      'momentum':[0.9], \n",
        "                                                      'batch_size': [16]})\n",
        "\n",
        "mlp_clf_opt.fit(dataset_conditioned_train.features, dataset_conditioned_train.labels.ravel())\n",
        "pred = mlp_clf_opt.predict(dataset_conditioned_validate_test.features)\n",
        "\n",
        "model_accuracy = performance.accuracy_score(dataset_conditioned_validate_test.labels, pred, normalize=True)*100\n",
        "print(' Accuracy of MLP on training set: {0:.2f}% '.format(model_accuracy))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2TW8b_6EjkIH"
      },
      "source": [
        "#### ROC curve for race"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FV04d7y8jkII"
      },
      "source": [
        "# ROC curve for black vs caucasian\n",
        "\n",
        "fpr_mlp_race,tpr_mlp_race,label_mlp_race=[],[],[]\n",
        "\n",
        "\n",
        "african_american_prediction = mlp_clf_opt.predict(african_american_samples)\n",
        "\n",
        "fpr_AM,tpr_AM,thresholds_AM = performance.roc_curve(african_american_samples_label.to_numpy().ravel(),\n",
        "                                           african_american_prediction,pos_label=1)\n",
        "\n",
        "fpr_mlp_race.append(fpr_AM)\n",
        "tpr_mlp_race.append(tpr_AM)\n",
        "label_mlp_race.append('ROC of african american')\n",
        "\n",
        "caucasian_prediction = mlp_clf_opt.predict(caucasian_samples)\n",
        "fpr_C,tpr_C,thresholds_C = performance.roc_curve(caucasian_samples_label.to_numpy().ravel(),\n",
        "                                           caucasian_prediction,pos_label=1)\n",
        "fpr_mlp_race.append(fpr_C)\n",
        "tpr_mlp_race.append(tpr_C)\n",
        "label_mlp_race.append('ROC of Caucasians')\n",
        "\n",
        "plot_roc(fpr_mlp_race,tpr_mlp_race,label=label_mlp_race)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVgSJH6qjkII"
      },
      "source": [
        "#### ROC curve for gender"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16j4EJeOjkII"
      },
      "source": [
        "# ROC curve for male vs female\n",
        "fpr_mlp_gender,tpr_mlp_gender,label_mlp_gender=[],[],[]\n",
        "\n",
        "\n",
        "female_prediction = mlp_clf_opt.predict(female_samples)\n",
        "\n",
        "fpr_female,tpr_female,thresholds_female = performance.roc_curve(female_samples_label.to_numpy().ravel(),\n",
        "                                           female_prediction,pos_label=1)\n",
        "\n",
        "fpr_mlp_gender.append(fpr_female)\n",
        "tpr_mlp_gender.append(tpr_female)\n",
        "label_mlp_gender.append('ROC of females')\n",
        "\n",
        "male_prediction = mlp_clf_opt.predict(male_samples)\n",
        "fpr_male,tpr_male,thresholds_male = performance.roc_curve(male_samples_label.to_numpy().ravel(),\n",
        "                                           male_prediction,pos_label=1)\n",
        "fpr_mlp_gender.append(fpr_male)\n",
        "tpr_mlp_gender.append(tpr_male)\n",
        "label_mlp_gender.append('ROC of male')\n",
        "\n",
        "plot_roc(fpr_mlp_gender,tpr_mlp_gender,label=label_mlp_gender)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLOn7Q3ijkIJ"
      },
      "source": [
        "### Decision trees"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AV-fcDDijkIJ"
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "tree_classifier = DecisionTreeClassifier()\n",
        "\n",
        "tree_clf_opt = GridSearchCV(tree_classifier,param_grid={'max_depth':[4,6,8,10,12]})\n",
        "tree_clf_opt.fit(dataset_conditioned_train.features, dataset_conditioned_train.labels.ravel())\n",
        "\n",
        "pred = tree_clf_opt.predict(dataset_conditioned_validate_test.features)\n",
        "\n",
        "print('Accuracy of the Decision tree on testing set {0:.2f}% '\n",
        "      .format(performance.accuracy_score(dataset_conditioned_validate_test.labels, pred, normalize=True)*100))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWgsRvhhjkIJ"
      },
      "source": [
        "#### ROC curve for race"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "kmvSOcvmjkIJ"
      },
      "source": [
        "# ROC curve for black vs caucasian\n",
        "\n",
        "fpr_Dec_tree_race,tpr_Dec_tree_race,label_Dec_tree_race=[],[],[]\n",
        "\n",
        "\n",
        "african_american_prediction = tree_clf_opt.predict(african_american_samples)\n",
        "\n",
        "fpr_AM,tpr_AM,thresholds_AM = performance.roc_curve(african_american_samples_label.to_numpy().ravel(),\n",
        "                                           african_american_prediction,pos_label=1)\n",
        "\n",
        "fpr_Dec_tree_race.append(fpr_AM)\n",
        "tpr_Dec_tree_race.append(tpr_AM)\n",
        "label_Dec_tree_race.append('ROC of african american')\n",
        "\n",
        "caucasian_prediction = tree_clf_opt.predict(caucasian_samples)\n",
        "fpr_C,tpr_C,thresholds_C = performance.roc_curve(caucasian_samples_label.to_numpy().ravel(),\n",
        "                                           caucasian_prediction,pos_label=1)\n",
        "fpr_Dec_tree_race.append(fpr_C)\n",
        "tpr_Dec_tree_race.append(tpr_C)\n",
        "label_Dec_tree_race.append('ROC of Caucasians')\n",
        "\n",
        "plot_roc(fpr_Dec_tree_race,tpr_Dec_tree_race,label=label_Dec_tree_race)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omFROkgdjkIJ"
      },
      "source": [
        "#### ROC curve for gender\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFueGn8vjkIK"
      },
      "source": [
        "# ROC curve for male vs female\n",
        "fpr_Dec_tree_gender,tpr_Dec_tree_gender,label_Dec_tree_gender=[],[],[]\n",
        "\n",
        "\n",
        "female_prediction = tree_clf_opt.predict(female_samples)\n",
        "\n",
        "fpr_female,tpr_female,thresholds_female = performance.roc_curve(female_samples_label.to_numpy().ravel(),\n",
        "                                           female_prediction,pos_label=1)\n",
        "\n",
        "fpr_Dec_tree_gender.append(fpr_female)\n",
        "tpr_Dec_tree_gender.append(tpr_female)\n",
        "label_Dec_tree_gender.append('ROC of females')\n",
        "\n",
        "male_prediction = tree_clf_opt.predict(male_samples)\n",
        "fpr_male,tpr_male,thresholds_male = performance.roc_curve(male_samples_label.to_numpy().ravel(),\n",
        "                                           male_prediction,pos_label=1)\n",
        "fpr_Dec_tree_gender.append(fpr_male)\n",
        "tpr_Dec_tree_gender.append(tpr_male)\n",
        "label_Dec_tree_gender.append('ROC of male')\n",
        "\n",
        "plot_roc(fpr_Dec_tree_gender,tpr_Dec_tree_gender,label=label_Dec_tree_gender)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFc1QZGtjkIK"
      },
      "source": [
        "### **Question 3: What biases did the classifiers show?**\n",
        "After training and testing these simple classifiers, what did you notice about the results above? How did they compare to the ROC curves from the COMPAS data? Answer these questions in the cell below"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtQA1g3kjkIK"
      },
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUI9FCt_sSRr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nryVxMw_jkIL"
      },
      "source": [
        "## Building fairer classifiers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLQPqbB8jkIL"
      },
      "source": [
        "### **Question 4: How can these classifiers be improved?**\n",
        "What modifications can be made on the above classifiers that would result in fairer outcomes? Write two different potential modifications that could be made in the cell below"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNW-kYdYKCI8"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P15azilhjkIQ"
      },
      "source": [
        "###**Extra Credit - Implement a Fairer classifier**\n",
        "Using your suggested modifications above, modify one of the simple classifiers in order to produce a fairer model. Implement this model and plot the resulting ROC curves below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ocjxkuy-Kxd1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}